{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Training \n",
    "\n",
    "Test CNN training vs simulated dataset size.  This uses a nearly-identical copy\n",
    "of Ulascan's code for generating the stim shots and training, just wrapped in\n",
    "some functions to make it easier to iterate over hyperparameters.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-24 11:11:12.285399: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-06-24 11:11:12.285436: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-06-24 11:11:14.159215: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-06-24 11:11:14.159433: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-06-24 11:11:14.159447: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "import time \n",
    "import os\n",
    "import json \n",
    "import itertools\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.optimizers import Adam\n",
    "import pymatching\n",
    "\n",
    "from circuit_generators import *\n",
    "from sampling_functions import *\n",
    "from bitpack import pack_bits, unpack_bits\n",
    "from circuit_partition import *\n",
    "from utilities_tf import *\n",
    "from CNNModel import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Surface code definitions\n",
    "These are used as globals in the stim and training functions below.  (I should\n",
    "update those functions to take these SC parameters as inputs, but this works\n",
    "for now.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of worker nodes\n",
    "n_worker_nodes = 7\n",
    "\n",
    "# Surface code specifications\n",
    "d = 5\n",
    "r = 2\n",
    "kernel_size = 3\n",
    "\n",
    "# Error probabilities\n",
    "p = 0.01\n",
    "before_round_data_depolarization = p\n",
    "after_reset_flip_probability = p\n",
    "after_clifford_depolarization = p\n",
    "before_measure_flip_probability = p\n",
    "\n",
    "use_rotated_z = True\n",
    "observable_type = \"ZL\" if use_rotated_z else \"XL\"\n",
    "\n",
    "# Bit types\n",
    "binary_t = np.int8 # Could use even less if numpy allowed\n",
    "\n",
    "# Measurement index type\n",
    "idx_t = np.int8\n",
    "n_all_measurements = r*(d**2-1) + d**2\n",
    "if n_all_measurements > np.iinfo(idx_t).max:\n",
    "  idx_t = np.int16\n",
    "if n_all_measurements > np.iinfo(idx_t).max:\n",
    "  idx_t = np.int32\n",
    "if n_all_measurements > np.iinfo(idx_t).max:\n",
    "  idx_t = np.int64\n",
    "if n_all_measurements > np.iinfo(idx_t).max:\n",
    "  idx_t = np.int128\n",
    "if n_all_measurements > np.iinfo(idx_t).max:\n",
    "  idx_t = np.int256\n",
    "if n_all_measurements > np.iinfo(idx_t).max:\n",
    "  raise RuntimeError(\"idx_t is too small.\")\n",
    "\n",
    "# Call signature for circuit_partition::group_det_bits_kxk\n",
    "call_group_det_bits_kxk = lambda det_bits_dxd, data_bits_dxd=None, d=d, r=r, k=kernel_size, use_rotated_z=use_rotated_z, binary_t=binary_t, idx_t=idx_t: group_det_bits_kxk(det_bits_dxd, d, r, k, use_rotated_z, data_bits_dxd, binary_t, idx_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stim and CNN training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_measurements(measurements, d):\n",
    "  n_measurements = idx_t(measurements.shape[1])\n",
    "  # Measurements on data qubits come last\n",
    "  exclude_indices = np.array([-x-1 for x in range(d**2)], dtype=idx_t)\n",
    "  exclude_indices = exclude_indices + n_measurements\n",
    "  # Out of all measurements on data qubits, the logical qubit measurements are those on the boundary of the lattice.\n",
    "  # All other equivalent X_L/Z_L operators can be found through the combination of ancilla measurements and the chosen data qubits giving us the logical qubit.\n",
    "  exclude_indices_obsL = np.array([-x-1 for x in range(d*(d-1), d**2)], dtype=idx_t)\n",
    "  exclude_indices_obsL = exclude_indices_obsL + n_measurements\n",
    "  # From obs_bits, we want to exclude all measurements except those listed in exclude_indices_obsL\n",
    "  exclude_indices_obs = np.arange(0, n_measurements, 1, dtype=idx_t)\n",
    "  exclude_indices_obs = np.delete(exclude_indices_obs, exclude_indices_obsL)\n",
    "\n",
    "  det_bits = measurements\n",
    "  det_bits = np.delete(det_bits, exclude_indices, axis=1)\n",
    "  obs_bits = measurements\n",
    "  obs_bits = np.delete(obs_bits, exclude_indices_obs, axis=1)\n",
    "\n",
    "  data_bits = measurements[:, exclude_indices]\n",
    "\n",
    "  # Reverse the order of data_bits because exclude_indices starts from the last data qubit measurement, not the first\n",
    "  data_bits = np.flip(data_bits, axis=1)\n",
    "\n",
    "  return det_bits, obs_bits, data_bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stim_output(n_total, split):\n",
    "  # this is using globals defined in the first cell!\n",
    "  n_test = int(n_total*split)\n",
    "  n_train = int(n_total*(1-split))\n",
    "  n_samples = n_test + n_train\n",
    "  decoders = ['pymatching']\n",
    "  test_circuit = get_builtin_circuit(\n",
    "    \"surface_code:rotated_memory_\"+('z' if use_rotated_z else 'x'),\n",
    "    distance=d,\n",
    "    rounds=r,\n",
    "    before_round_data_depolarization = before_round_data_depolarization,\n",
    "    after_reset_flip_probability = after_reset_flip_probability,\n",
    "    after_clifford_depolarization = after_clifford_depolarization,\n",
    "    before_measure_flip_probability = before_measure_flip_probability\n",
    "  )\n",
    "\n",
    "  kernel_circuit_extra_depol1 = [\n",
    "    [\n",
    "      f\"DEPOLARIZE1({after_clifford_depolarization}) 13 25\",\n",
    "      f\"DEPOLARIZE1({after_clifford_depolarization}) 5 13 17 19 25\",\n",
    "      f\"#DEPOLARIZE1({after_clifford_depolarization})\",\n",
    "      f\"DEPOLARIZE1({after_clifford_depolarization}) 12 15 19\",\n",
    "    ], # parity = (1, 1)\n",
    "    [\n",
    "      f\"DEPOLARIZE1({after_clifford_depolarization}) 8 13 25\",\n",
    "      f\"DEPOLARIZE1({after_clifford_depolarization}) 5 13 17 19 25\",\n",
    "      f\"DEPOLARIZE1({after_clifford_depolarization}) 1 14 15\",\n",
    "      f\"DEPOLARIZE1({after_clifford_depolarization}) 12 14 15 19\",\n",
    "    ], # parity = (0, 1)\n",
    "    [\n",
    "      f\"DEPOLARIZE1({after_clifford_depolarization}) 8 25\",\n",
    "      f\"DEPOLARIZE1({after_clifford_depolarization}) 17 25\",\n",
    "      f\"DEPOLARIZE1({after_clifford_depolarization}) 1 14 15\",\n",
    "      f\"DEPOLARIZE1({after_clifford_depolarization}) 14 15\",\n",
    "    ], # parity = (-1, 1)\n",
    "    [\n",
    "      f\"DEPOLARIZE1({after_clifford_depolarization}) 5 13 25\",\n",
    "      f\"DEPOLARIZE1({after_clifford_depolarization}) 5 13 17 19 25\",\n",
    "      f\"DEPOLARIZE1({after_clifford_depolarization}) 2 3\",\n",
    "      f\"DEPOLARIZE1({after_clifford_depolarization}) 2 12 15 19\",\n",
    "    ], # parity = (1, 0)\n",
    "    [\n",
    "      f\"DEPOLARIZE1({after_clifford_depolarization}) 1 5 8 13 25\",\n",
    "      f\"DEPOLARIZE1({after_clifford_depolarization}) 5 13 17 19 25\",\n",
    "      f\"DEPOLARIZE1({after_clifford_depolarization}) 1 2 3 14 15\",\n",
    "      f\"DEPOLARIZE1({after_clifford_depolarization}) 2 12 14 15 19\",\n",
    "    ] # parity = (0, 0)\n",
    "  ]\n",
    "  kernel_circuits = []\n",
    "  for replace_args in kernel_circuit_extra_depol1:\n",
    "    kernel_circuit_template = \\\n",
    "    f\"\"\"\n",
    "  QUBIT_COORDS(1, 1) 1\n",
    "  QUBIT_COORDS(2, 0) 2\n",
    "  QUBIT_COORDS(3, 1) 3\n",
    "  QUBIT_COORDS(5, 1) 5\n",
    "  QUBIT_COORDS(1, 3) 8\n",
    "  QUBIT_COORDS(2, 2) 9\n",
    "  QUBIT_COORDS(3, 3) 10\n",
    "  QUBIT_COORDS(4, 2) 11\n",
    "  QUBIT_COORDS(5, 3) 12\n",
    "  QUBIT_COORDS(6, 2) 13\n",
    "  QUBIT_COORDS(0, 4) 14\n",
    "  QUBIT_COORDS(1, 5) 15\n",
    "  QUBIT_COORDS(2, 4) 16\n",
    "  QUBIT_COORDS(3, 5) 17\n",
    "  QUBIT_COORDS(4, 4) 18\n",
    "  QUBIT_COORDS(5, 5) 19\n",
    "  QUBIT_COORDS(4, 6) 25\n",
    "  R 1 3 5 8 10 12 15 17 19\n",
    "  X_ERROR({after_reset_flip_probability}) 1 3 5 8 10 12 15 17 19\n",
    "  R 2 9 11 13 14 16 18 25\n",
    "  X_ERROR({after_reset_flip_probability}) 2 9 11 13 14 16 18 25\n",
    "  TICK\n",
    "  DEPOLARIZE1({before_round_data_depolarization}) 1 3 5 8 10 12 15 17 19\n",
    "  H 2 11 16 25\n",
    "  DEPOLARIZE1({after_clifford_depolarization}) 2 11 16 25\n",
    "  TICK\n",
    "  CX 2 3 16 17 11 12 15 14 10 9 19 18\n",
    "  DEPOLARIZE2({after_clifford_depolarization}) 2 3 16 17 11 12 15 14 10 9 19 18\n",
    "  {replace_args[0]}\n",
    "  TICK\n",
    "  CX 2 1 16 15 11 10 8 14 3 9 12 18\n",
    "  DEPOLARIZE2({after_clifford_depolarization}) 2 1 16 15 11 10 8 14 3 9 12 18\n",
    "  {replace_args[1]}\n",
    "  TICK\n",
    "  CX 16 10 11 5 25 19 8 9 17 18 12 13\n",
    "  DEPOLARIZE2({after_clifford_depolarization}) 16 10 11 5 25 19 8 9 17 18 12 13\n",
    "  {replace_args[2]}\n",
    "  TICK\n",
    "  CX 16 8 11 3 25 17 1 9 10 18 5 13\n",
    "  DEPOLARIZE2({after_clifford_depolarization}) 16 8 11 3 25 17 1 9 10 18 5 13\n",
    "  {replace_args[3]}\n",
    "  TICK\n",
    "  H 2 11 16 25\n",
    "  DEPOLARIZE1({after_clifford_depolarization}) 2 11 16 25\n",
    "  TICK\n",
    "  X_ERROR({before_measure_flip_probability}) 2 9 11 13 14 16 18 25\n",
    "  MR 2 9 11 13 14 16 18 25\n",
    "  X_ERROR({after_reset_flip_probability}) 2 9 11 13 14 16 18 25\n",
    "  DETECTOR(0, 4, 0) rec[-4]\n",
    "  DETECTOR(2, 2, 0) rec[-7]\n",
    "  DETECTOR(4, 4, 0) rec[-2]\n",
    "  DETECTOR(6, 2, 0) rec[-5]\n",
    "  REPEAT {r-1} {{\n",
    "    TICK\n",
    "    DEPOLARIZE1({before_round_data_depolarization}) 1 3 5 8 10 12 15 17 19\n",
    "    H 2 11 16 25\n",
    "    DEPOLARIZE1({after_clifford_depolarization}) 2 11 16 25\n",
    "    TICK\n",
    "    CX 2 3 16 17 11 12 15 14 10 9 19 18\n",
    "    DEPOLARIZE2({after_clifford_depolarization}) 2 3 16 17 11 12 15 14 10 9 19 18\n",
    "    {replace_args[0]}\n",
    "    TICK\n",
    "    CX 2 1 16 15 11 10 8 14 3 9 12 18\n",
    "    DEPOLARIZE2({after_clifford_depolarization}) 2 1 16 15 11 10 8 14 3 9 12 18\n",
    "    {replace_args[1]}\n",
    "    TICK\n",
    "    CX 16 10 11 5 25 19 8 9 17 18 12 13\n",
    "    DEPOLARIZE2({after_clifford_depolarization}) 16 10 11 5 25 19 8 9 17 18 12 13\n",
    "    {replace_args[2]}\n",
    "    TICK\n",
    "    CX 16 8 11 3 25 17 1 9 10 18 5 13\n",
    "    DEPOLARIZE2({after_clifford_depolarization}) 16 8 11 3 25 17 1 9 10 18 5 13\n",
    "    {replace_args[3]}\n",
    "    TICK\n",
    "    H 2 11 16 25\n",
    "    DEPOLARIZE1({after_clifford_depolarization}) 2 11 16 25\n",
    "    TICK\n",
    "    X_ERROR({before_measure_flip_probability}) 2 9 11 13 14 16 18 25\n",
    "    MR 2 9 11 13 14 16 18 25\n",
    "    X_ERROR({after_reset_flip_probability}) 2 9 11 13 14 16 18 25\n",
    "    SHIFT_COORDS(0, 0, 1)\n",
    "    DETECTOR(2, 0, 0) rec[-8] rec[-16]\n",
    "    DETECTOR(2, 2, 0) rec[-7] rec[-15]\n",
    "    DETECTOR(4, 2, 0) rec[-6] rec[-14]\n",
    "    DETECTOR(6, 2, 0) rec[-5] rec[-13]\n",
    "    DETECTOR(0, 4, 0) rec[-4] rec[-12]\n",
    "    DETECTOR(2, 4, 0) rec[-3] rec[-11]\n",
    "    DETECTOR(4, 4, 0) rec[-2] rec[-10]\n",
    "    DETECTOR(4, 6, 0) rec[-1] rec[-9]\n",
    "  }}\n",
    "  X_ERROR({before_measure_flip_probability}) 1 3 5 8 10 12 15 17 19\n",
    "  M 1 3 5 8 10 12 15 17 19\n",
    "  DETECTOR(0, 4, 1) rec[-3] rec[-6] rec[-13]\n",
    "  DETECTOR(2, 2, 1) rec[-5] rec[-6] rec[-8] rec[-9] rec[-16]\n",
    "  DETECTOR(4, 4, 1) rec[-1] rec[-2] rec[-4] rec[-5] rec[-11]\n",
    "  DETECTOR(6, 2, 1) rec[-4] rec[-7] rec[-14]\n",
    "  OBSERVABLE_INCLUDE(0) rec[-7] rec[-8] rec[-9]\n",
    "    \"\"\"\n",
    "    kernel_circuits.append(stim.Circuit(kernel_circuit_template))\n",
    "\n",
    "\n",
    "  # Sampling for the dxd circuit\n",
    "  m_sampler = test_circuit.compile_sampler(seed=12345)\n",
    "  converter = test_circuit.compile_m2d_converter()\n",
    "  detector_error_model = test_circuit.detector_error_model(decompose_errors=True)\n",
    "\n",
    "  measurements = m_sampler.sample(n_samples, bit_packed=False)\n",
    "  det_evts, flips = converter.convert(measurements=measurements, separate_observables=True, bit_packed=False)\n",
    "  measurements = measurements.astype(binary_t)\n",
    "  det_evts = det_evts.astype(binary_t)\n",
    "  flips = flips.astype(binary_t)\n",
    "\n",
    "  avg_flips = np.sum(flips.reshape(-1,), dtype=np.float32)/flips.shape[0]\n",
    "  print(f\"Average flip rate for the full circuit: {avg_flips}\")\n",
    "\n",
    "  # next cell\n",
    "  n_measurements = idx_t(measurements.shape[1])\n",
    "  det_bits, obs_bits, data_bits = split_measurements(measurements, d)\n",
    "\n",
    "  # next cell\n",
    "  det_bits_kxk_all, data_bits_kxk_all, obs_bits_kxk_all, kernel_result_translation_map = call_group_det_bits_kxk(det_bits, data_bits_dxd=data_bits)\n",
    "  kernel_types = get_unique_kernel_types(kernel_size, d)\n",
    "  n_kernels = det_bits_kxk_all.shape[0]\n",
    "\n",
    "  # next cell\n",
    "  kernel_result_translation_map_f = kernel_result_translation_map[:,:,1:]\n",
    "  kernel_result_translation_map_b = kernel_result_translation_map[:,:,0:-1]\n",
    "  kernel_result_translation_det_evts = (kernel_result_translation_map_f!=kernel_result_translation_map_b).astype(binary_t)\n",
    "  kernel_result_translation_map = np.concatenate((kernel_result_translation_map, kernel_result_translation_det_evts), axis=2)\n",
    "\n",
    "  # next cell\n",
    "  det_evts_kxk_all = []\n",
    "  flips_kxk_all = []\n",
    "  converters_kernel = []\n",
    "  for kernel_circuit in kernel_circuits:\n",
    "    converters_kernel.append(kernel_circuit.compile_m2d_converter())\n",
    "  for k in range(n_kernels):\n",
    "    measurements_kxk = np.concatenate((det_bits_kxk_all[k], data_bits_kxk_all[k]), axis=1).astype(np.bool_)\n",
    "    ik = 0\n",
    "    for i, kernel_type in enumerate(kernel_types):\n",
    "      if k in kernel_type[1]:\n",
    "        ik = i\n",
    "        break\n",
    "    det_evts_kxk, flips_kxk = converters_kernel[ik].convert(measurements=measurements_kxk, separate_observables=True, bit_packed=False)\n",
    "    det_evts_kxk_all.append(det_evts_kxk)\n",
    "    flips_kxk_all.append(flips_kxk)\n",
    "  det_evts_kxk_all = np.array(det_evts_kxk_all, dtype=binary_t)\n",
    "  flips_kxk_all = np.array(flips_kxk_all, dtype=binary_t)\n",
    "  del converters_kernel\n",
    "\n",
    "  # next cell\n",
    "  class_bits = flips\n",
    "  features_det_bits = np.swapaxes(det_bits_kxk_all, 0, 1)\n",
    "  features_det_evts = np.swapaxes(det_evts_kxk_all, 0, 1)\n",
    "  features_translation_map = np.swapaxes(kernel_result_translation_map, 0, 1) # Dimensions go as [sample][kernel][cycle bits + detections (n_cycles-1)]\n",
    "  features_translation_map = np.reshape(features_translation_map, (features_translation_map.shape[0], -1))\n",
    "  features_final_det_evts = det_evts[:, -((d**2-1)//2):]\n",
    "\n",
    "  return {\"CNN\": [[features_det_bits,\n",
    "                   features_det_evts,\n",
    "                   features_translation_map,\n",
    "                   features_final_det_evts],\n",
    "                  class_bits], \n",
    "          \"pymatch\": [detector_error_model, det_evts, flips]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pymatching_result(stim_output, idxs_test):\n",
    "    detector_error_model, det_evts, flips = stim_output[\"pymatch\"]\n",
    "    pymatcher = pymatching.Matching.from_detector_error_model(detector_error_model)\n",
    "    flips_pred_pym = pymatcher.decode_batch(det_evts, bit_packed_predictions=False, bit_packed_shots=False).astype(binary_t).reshape(-1,1)\n",
    "    flips_pred_pym = pymatcher.decode_batch(det_evts[idxs_test,:], bit_packed_predictions=False, bit_packed_shots=False).astype(binary_t).reshape(-1,1)\n",
    "    pymatch_error_rate_test = np.sum(\n",
    "        (flips_pred_pym!=flips[idxs_test,:]))/flips_pred_pym.shape[0]\n",
    "    return pymatch_error_rate_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = \"sc-CNN-training\"\n",
    "run = \"testing\" #\"in_memory\"\n",
    "n_totals = [10**4, 10**5] #[10**5, 10**6, 10**7]\n",
    "frac_test = 0.2\n",
    "batch_size = 10**3  \n",
    "n_epochs = 20 # 60\n",
    "n_nodes = 150\n",
    "lrs = [0.005, 0.007] #[0.05, 0.005, 0.0005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_params = {\n",
    "  \"d\": d,\n",
    "  \"r\": r,\n",
    "  \"kernel_size\": kernel_size,\n",
    "  \"p\": p,\n",
    "  \"before_round_data_depolarization\": before_round_data_depolarization,\n",
    "  \"after_reset_flip_probability\": after_reset_flip_probability,\n",
    "  \"after_clifford_depolarization\": after_clifford_depolarization,\n",
    "  \"before_measure_flip_probability\": before_measure_flip_probability,\n",
    "  \"use_rotated_z\": use_rotated_z,\n",
    "  \"observable_type\": observable_type,\n",
    "  \"runname\": run,\n",
    "  \"n_totals\": n_totals,\n",
    "  \"frac_test\": frac_test,\n",
    "  \"batch_size\": batch_size,\n",
    "  \"n_epochs\": n_epochs,\n",
    "  \"n_nodes\": n_nodes,\n",
    "  \"lrs\": lrs,\n",
    "}\n",
    "\n",
    "setupfile_path = f\"{base}/{run}/setup.json\"\n",
    "if not os.path.exists(setupfile_path):\n",
    "    os.makedirs(f\"{base}/{run}\", exist_ok=True)\n",
    "with open(setupfile_path, 'w') as f: json.dump(setup_params, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run stim and pymatching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average flip rate for the full circuit: 0.22601\n",
      "PyMatching error rate for full data set: 3.1740%\n"
     ]
    }
   ],
   "source": [
    "# run stim for largest desired sample size\n",
    "max_total = np.max(n_totals)\n",
    "stim_output = get_stim_output(max_total, frac_test)\n",
    "[[features_det_bits,\n",
    "  features_det_evts,\n",
    "  features_translation_map,\n",
    "  features_final_det_evts], class_bits] = stim_output[\"CNN\"]\n",
    "\n",
    "# PyMatching error rate\n",
    "pymatch_error_rate = get_pymatching_result(\n",
    "  stim_output, np.arange(max_total, dtype=np.int32))\n",
    "flit_rate = stim_output[\"pymatch\"][2].sum()/stim_output[\"pymatch\"][2].shape[0]\n",
    "print(f\"PyMatching error rate for full data set: \"\n",
    "      f\"{100*pymatch_error_rate:0.4f}%\")\n",
    "\n",
    "# save\n",
    "pymatch_file = f\"{base}/{run}/pymatch_results.json\"\n",
    "pymatch_results = {\"flip_rate\": flit_rate, \"error_rate\": pymatch_error_rate}\n",
    "with open(pymatch_file, 'w') as f: json.dump(pymatch_results, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train CNNs\n",
    "\n",
    "This trains the models defined above, and records the training history.  \n",
    "\n",
    "(I would like to save the actual models, but the built-in functions to do this\n",
    "fail here since the CNN here is built as a custom class. It seems we need to add\n",
    "code to those classes which shows Keras how to serialize the model.  That got\n",
    "complicated, so for now just save the training history and we'll need to re-train\n",
    "if we want these models for anything.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training CNN model 1/4\n",
      "    n_total = 1.0e+04 samples\n",
      "         lr = 0.005\n",
      "----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-24 11:11:18.605617: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2024-06-24 11:11:18.606234: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2024-06-24 11:11:18.606325: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (rjanish-ThinkPad-X220): /proc/driver/nvidia/version does not exist\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique contributions: 13\n",
      "Total number of fractions: 28\n",
      "Total number of phases: 62\n",
      "Epoch 1/20\n",
      "2/2 [==============================] - 30s 5s/step - loss: 0.6071 - accuracy: 0.6215 - val_loss: 0.7647 - val_accuracy: 0.7653\n",
      "Epoch 2/20\n",
      "2/2 [==============================] - 1s 426ms/step - loss: 0.6188 - accuracy: 0.7855 - val_loss: 0.5375 - val_accuracy: 0.7653\n",
      "Epoch 3/20\n",
      "2/2 [==============================] - 0s 351ms/step - loss: 0.5336 - accuracy: 0.7855 - val_loss: 0.5390 - val_accuracy: 0.7653\n",
      "Epoch 4/20\n",
      "2/2 [==============================] - 0s 350ms/step - loss: 0.5094 - accuracy: 0.7855 - val_loss: 0.5412 - val_accuracy: 0.7653\n",
      "Epoch 5/20\n",
      "2/2 [==============================] - 0s 352ms/step - loss: 0.5059 - accuracy: 0.7855 - val_loss: 0.5251 - val_accuracy: 0.7653\n",
      "Epoch 6/20\n",
      "2/2 [==============================] - 0s 345ms/step - loss: 0.4874 - accuracy: 0.7855 - val_loss: 0.5033 - val_accuracy: 0.7653\n",
      "Epoch 7/20\n",
      "2/2 [==============================] - 0s 349ms/step - loss: 0.4776 - accuracy: 0.7855 - val_loss: 0.4921 - val_accuracy: 0.7654\n",
      "Epoch 8/20\n",
      "2/2 [==============================] - 0s 346ms/step - loss: 0.4634 - accuracy: 0.7860 - val_loss: 0.4887 - val_accuracy: 0.7651\n",
      "Epoch 9/20\n",
      "2/2 [==============================] - 0s 359ms/step - loss: 0.4566 - accuracy: 0.7850 - val_loss: 0.4780 - val_accuracy: 0.7655\n",
      "Epoch 10/20\n",
      "2/2 [==============================] - 0s 347ms/step - loss: 0.4453 - accuracy: 0.7920 - val_loss: 0.4642 - val_accuracy: 0.7709\n",
      "Epoch 11/20\n",
      "2/2 [==============================] - 1s 486ms/step - loss: 0.4373 - accuracy: 0.8000 - val_loss: 0.4565 - val_accuracy: 0.7754\n",
      "Epoch 12/20\n",
      "2/2 [==============================] - 0s 422ms/step - loss: 0.4273 - accuracy: 0.7995 - val_loss: 0.4567 - val_accuracy: 0.7761\n",
      "Epoch 13/20\n",
      "2/2 [==============================] - 0s 413ms/step - loss: 0.4181 - accuracy: 0.8020 - val_loss: 0.4493 - val_accuracy: 0.7818\n",
      "Epoch 14/20\n",
      "2/2 [==============================] - 0s 353ms/step - loss: 0.4117 - accuracy: 0.7985 - val_loss: 0.4471 - val_accuracy: 0.7853\n",
      "Epoch 15/20\n",
      "2/2 [==============================] - 0s 427ms/step - loss: 0.4017 - accuracy: 0.8015 - val_loss: 0.4433 - val_accuracy: 0.7859\n",
      "Epoch 16/20\n",
      "2/2 [==============================] - 0s 372ms/step - loss: 0.3947 - accuracy: 0.8080 - val_loss: 0.4384 - val_accuracy: 0.7885\n",
      "Epoch 17/20\n",
      "2/2 [==============================] - 0s 343ms/step - loss: 0.3842 - accuracy: 0.8170 - val_loss: 0.4364 - val_accuracy: 0.7885\n",
      "Epoch 18/20\n",
      "2/2 [==============================] - 0s 350ms/step - loss: 0.3693 - accuracy: 0.8250 - val_loss: 0.4336 - val_accuracy: 0.7872\n",
      "Epoch 19/20\n",
      "2/2 [==============================] - 0s 346ms/step - loss: 0.3593 - accuracy: 0.8330 - val_loss: 0.4411 - val_accuracy: 0.7859\n",
      "Epoch 20/20\n",
      "2/2 [==============================] - 0s 345ms/step - loss: 0.3471 - accuracy: 0.8360 - val_loss: 0.4375 - val_accuracy: 0.7846\n",
      "\n",
      "Finished training CNN model 1/4\n",
      "    n_total = 1.0e+04 samples\n",
      "         lr = 0.005\n",
      "Best test error rate was 21.1500% in epoch 16/20\n",
      "\n",
      "Training CNN model 2/4\n",
      "    n_total = 1.0e+04 samples\n",
      "         lr = 0.007\n",
      "----------------------------\n",
      "Number of unique contributions: 13\n",
      "Total number of fractions: 28\n",
      "Total number of phases: 62\n",
      "Epoch 1/20\n",
      "2/2 [==============================] - 25s 5s/step - loss: 0.6194 - accuracy: 0.6840 - val_loss: 0.5345 - val_accuracy: 0.7653\n",
      "Epoch 2/20\n",
      "2/2 [==============================] - 0s 361ms/step - loss: 0.4997 - accuracy: 0.7855 - val_loss: 0.5081 - val_accuracy: 0.7653\n",
      "Epoch 3/20\n",
      "2/2 [==============================] - 0s 340ms/step - loss: 0.4814 - accuracy: 0.7840 - val_loss: 0.5073 - val_accuracy: 0.7651\n",
      "Epoch 4/20\n",
      "2/2 [==============================] - 0s 418ms/step - loss: 0.4667 - accuracy: 0.7840 - val_loss: 0.4744 - val_accuracy: 0.7638\n",
      "Epoch 5/20\n",
      "2/2 [==============================] - 0s 413ms/step - loss: 0.4449 - accuracy: 0.7865 - val_loss: 0.4749 - val_accuracy: 0.7641\n",
      "Epoch 6/20\n",
      "2/2 [==============================] - 1s 464ms/step - loss: 0.4321 - accuracy: 0.7900 - val_loss: 0.4605 - val_accuracy: 0.7768\n",
      "Epoch 7/20\n",
      "2/2 [==============================] - 0s 364ms/step - loss: 0.4229 - accuracy: 0.7965 - val_loss: 0.4665 - val_accuracy: 0.7760\n",
      "Epoch 8/20\n",
      "2/2 [==============================] - 0s 343ms/step - loss: 0.4155 - accuracy: 0.7985 - val_loss: 0.4487 - val_accuracy: 0.7788\n",
      "Epoch 9/20\n",
      "2/2 [==============================] - 0s 420ms/step - loss: 0.3955 - accuracy: 0.8075 - val_loss: 0.4607 - val_accuracy: 0.7765\n",
      "Epoch 10/20\n",
      "2/2 [==============================] - 0s 420ms/step - loss: 0.3864 - accuracy: 0.8100 - val_loss: 0.4454 - val_accuracy: 0.7811\n",
      "Epoch 11/20\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 0.3721 - accuracy: 0.8145 - val_loss: 0.4444 - val_accuracy: 0.7816\n",
      "Epoch 12/20\n",
      "2/2 [==============================] - 1s 446ms/step - loss: 0.3533 - accuracy: 0.8215 - val_loss: 0.4472 - val_accuracy: 0.7786\n",
      "Epoch 13/20\n",
      "2/2 [==============================] - 0s 393ms/step - loss: 0.3410 - accuracy: 0.8300 - val_loss: 0.4554 - val_accuracy: 0.7789\n",
      "Epoch 14/20\n",
      "2/2 [==============================] - 0s 375ms/step - loss: 0.3222 - accuracy: 0.8515 - val_loss: 0.4779 - val_accuracy: 0.7805\n",
      "Epoch 15/20\n",
      "2/2 [==============================] - 0s 380ms/step - loss: 0.3063 - accuracy: 0.8615 - val_loss: 0.4714 - val_accuracy: 0.7763\n",
      "Epoch 16/20\n",
      "2/2 [==============================] - 1s 455ms/step - loss: 0.2839 - accuracy: 0.8680 - val_loss: 0.4908 - val_accuracy: 0.7749\n",
      "Epoch 17/20\n",
      "2/2 [==============================] - 1s 428ms/step - loss: 0.2668 - accuracy: 0.8840 - val_loss: 0.5118 - val_accuracy: 0.7788\n",
      "Epoch 18/20\n",
      "2/2 [==============================] - 1s 508ms/step - loss: 0.2435 - accuracy: 0.8935 - val_loss: 0.5304 - val_accuracy: 0.7789\n",
      "Epoch 19/20\n",
      "2/2 [==============================] - 0s 396ms/step - loss: 0.2222 - accuracy: 0.9055 - val_loss: 0.5974 - val_accuracy: 0.7834\n",
      "Epoch 20/20\n",
      "2/2 [==============================] - 1s 541ms/step - loss: 0.2169 - accuracy: 0.9100 - val_loss: 0.6290 - val_accuracy: 0.7820\n",
      "\n",
      "Finished training CNN model 2/4\n",
      "    n_total = 1.0e+04 samples\n",
      "         lr = 0.007\n",
      "Best test error rate was 21.6625% in epoch 19/20\n",
      "\n",
      "Training CNN model 3/4\n",
      "    n_total = 1.0e+05 samples\n",
      "         lr = 0.005\n",
      "----------------------------\n",
      "Number of unique contributions: 13\n",
      "Total number of fractions: 28\n",
      "Total number of phases: 62\n",
      "Epoch 1/20\n",
      "20/20 [==============================] - 31s 433ms/step - loss: 0.4876 - accuracy: 0.7794 - val_loss: 0.4430 - val_accuracy: 0.7810\n",
      "Epoch 2/20\n",
      "20/20 [==============================] - 4s 219ms/step - loss: 0.4121 - accuracy: 0.7872 - val_loss: 0.3876 - val_accuracy: 0.7918\n",
      "Epoch 3/20\n",
      "20/20 [==============================] - 4s 216ms/step - loss: 0.3606 - accuracy: 0.8150 - val_loss: 0.3451 - val_accuracy: 0.8362\n",
      "Epoch 4/20\n",
      "20/20 [==============================] - 4s 224ms/step - loss: 0.3171 - accuracy: 0.8565 - val_loss: 0.3190 - val_accuracy: 0.8600\n",
      "Epoch 5/20\n",
      "20/20 [==============================] - 4s 222ms/step - loss: 0.2874 - accuracy: 0.8741 - val_loss: 0.2916 - val_accuracy: 0.8761\n",
      "Epoch 6/20\n",
      "20/20 [==============================] - 4s 219ms/step - loss: 0.2620 - accuracy: 0.8864 - val_loss: 0.2807 - val_accuracy: 0.8775\n",
      "Epoch 7/20\n",
      "20/20 [==============================] - 4s 221ms/step - loss: 0.2401 - accuracy: 0.8978 - val_loss: 0.2577 - val_accuracy: 0.8912\n",
      "Epoch 8/20\n",
      "20/20 [==============================] - 5s 281ms/step - loss: 0.2199 - accuracy: 0.9071 - val_loss: 0.2491 - val_accuracy: 0.8954\n",
      "Epoch 9/20\n",
      "20/20 [==============================] - 7s 357ms/step - loss: 0.2009 - accuracy: 0.9157 - val_loss: 0.2336 - val_accuracy: 0.8992\n",
      "Epoch 10/20\n",
      "20/20 [==============================] - 4s 188ms/step - loss: 0.1851 - accuracy: 0.9226 - val_loss: 0.2397 - val_accuracy: 0.9000\n",
      "Epoch 11/20\n",
      "20/20 [==============================] - 4s 199ms/step - loss: 0.1745 - accuracy: 0.9272 - val_loss: 0.2328 - val_accuracy: 0.9048\n",
      "Epoch 12/20\n",
      "20/20 [==============================] - 7s 351ms/step - loss: 0.1614 - accuracy: 0.9331 - val_loss: 0.2428 - val_accuracy: 0.8980\n",
      "Epoch 13/20\n",
      "20/20 [==============================] - 4s 182ms/step - loss: 0.1546 - accuracy: 0.9359 - val_loss: 0.2349 - val_accuracy: 0.9026\n",
      "Epoch 14/20\n",
      "20/20 [==============================] - 7s 346ms/step - loss: 0.1382 - accuracy: 0.9433 - val_loss: 0.2391 - val_accuracy: 0.9039\n",
      "Epoch 15/20\n",
      "20/20 [==============================] - 4s 191ms/step - loss: 0.1258 - accuracy: 0.9491 - val_loss: 0.2341 - val_accuracy: 0.9081\n",
      "Epoch 16/20\n",
      "20/20 [==============================] - 7s 340ms/step - loss: 0.1192 - accuracy: 0.9530 - val_loss: 0.2747 - val_accuracy: 0.8911\n",
      "Epoch 17/20\n",
      "20/20 [==============================] - 4s 214ms/step - loss: 0.1203 - accuracy: 0.9513 - val_loss: 0.2505 - val_accuracy: 0.9025\n",
      "Epoch 18/20\n",
      "20/20 [==============================] - 5s 246ms/step - loss: 0.1034 - accuracy: 0.9602 - val_loss: 0.2613 - val_accuracy: 0.9037\n",
      "Epoch 19/20\n",
      "20/20 [==============================] - 4s 231ms/step - loss: 0.0951 - accuracy: 0.9624 - val_loss: 0.2706 - val_accuracy: 0.9040\n",
      "Epoch 20/20\n",
      "20/20 [==============================] - 5s 271ms/step - loss: 0.0913 - accuracy: 0.9634 - val_loss: 0.2865 - val_accuracy: 0.9062\n",
      "\n",
      "Finished training CNN model 3/4\n",
      "    n_total = 1.0e+05 samples\n",
      "         lr = 0.005\n",
      "Best test error rate was 9.1925% in epoch 15/20\n",
      "\n",
      "Training CNN model 4/4\n",
      "    n_total = 1.0e+05 samples\n",
      "         lr = 0.007\n",
      "----------------------------\n",
      "Number of unique contributions: 13\n",
      "Total number of fractions: 28\n",
      "Total number of phases: 62\n",
      "Epoch 1/20\n",
      "20/20 [==============================] - 29s 487ms/step - loss: 0.5291 - accuracy: 0.7466 - val_loss: 0.4564 - val_accuracy: 0.7713\n",
      "Epoch 2/20\n",
      "20/20 [==============================] - 5s 254ms/step - loss: 0.4346 - accuracy: 0.7876 - val_loss: 0.4157 - val_accuracy: 0.7880\n",
      "Epoch 3/20\n",
      "20/20 [==============================] - 7s 344ms/step - loss: 0.3885 - accuracy: 0.7989 - val_loss: 0.3697 - val_accuracy: 0.8140\n",
      "Epoch 4/20\n",
      "20/20 [==============================] - 4s 195ms/step - loss: 0.3400 - accuracy: 0.8382 - val_loss: 0.3284 - val_accuracy: 0.8425\n",
      "Epoch 5/20\n",
      "20/20 [==============================] - 4s 209ms/step - loss: 0.3085 - accuracy: 0.8574 - val_loss: 0.3161 - val_accuracy: 0.8505\n",
      "Epoch 6/20\n",
      "20/20 [==============================] - 5s 243ms/step - loss: 0.2836 - accuracy: 0.8723 - val_loss: 0.2895 - val_accuracy: 0.8728\n",
      "Epoch 7/20\n",
      "20/20 [==============================] - 7s 348ms/step - loss: 0.2621 - accuracy: 0.8863 - val_loss: 0.2787 - val_accuracy: 0.8804\n",
      "Epoch 8/20\n",
      "20/20 [==============================] - 4s 212ms/step - loss: 0.2399 - accuracy: 0.8982 - val_loss: 0.2562 - val_accuracy: 0.8888\n",
      "Epoch 9/20\n",
      "20/20 [==============================] - 5s 239ms/step - loss: 0.2153 - accuracy: 0.9082 - val_loss: 0.2510 - val_accuracy: 0.8930\n",
      "Epoch 10/20\n",
      "20/20 [==============================] - 4s 219ms/step - loss: 0.2018 - accuracy: 0.9168 - val_loss: 0.2479 - val_accuracy: 0.8945\n",
      "Epoch 11/20\n",
      "20/20 [==============================] - 5s 232ms/step - loss: 0.1857 - accuracy: 0.9201 - val_loss: 0.2415 - val_accuracy: 0.9007\n",
      "Epoch 12/20\n",
      "20/20 [==============================] - 7s 345ms/step - loss: 0.1686 - accuracy: 0.9283 - val_loss: 0.2456 - val_accuracy: 0.8952\n",
      "Epoch 13/20\n",
      "20/20 [==============================] - 4s 206ms/step - loss: 0.1670 - accuracy: 0.9287 - val_loss: 0.2490 - val_accuracy: 0.9002\n",
      "Epoch 14/20\n",
      "20/20 [==============================] - 4s 211ms/step - loss: 0.1538 - accuracy: 0.9363 - val_loss: 0.2515 - val_accuracy: 0.8995\n",
      "Epoch 15/20\n",
      "20/20 [==============================] - 7s 367ms/step - loss: 0.1408 - accuracy: 0.9406 - val_loss: 0.2551 - val_accuracy: 0.9006\n",
      "Epoch 16/20\n",
      "20/20 [==============================] - 6s 333ms/step - loss: 0.1298 - accuracy: 0.9458 - val_loss: 0.2569 - val_accuracy: 0.9000\n",
      "Epoch 17/20\n",
      "20/20 [==============================] - 4s 194ms/step - loss: 0.1230 - accuracy: 0.9500 - val_loss: 0.2724 - val_accuracy: 0.8964\n",
      "Epoch 18/20\n",
      "20/20 [==============================] - 4s 220ms/step - loss: 0.1143 - accuracy: 0.9549 - val_loss: 0.2982 - val_accuracy: 0.8891\n",
      "Epoch 19/20\n",
      "20/20 [==============================] - 4s 214ms/step - loss: 0.1118 - accuracy: 0.9539 - val_loss: 0.3050 - val_accuracy: 0.8975\n",
      "Epoch 20/20\n",
      "20/20 [==============================] - 5s 263ms/step - loss: 0.1122 - accuracy: 0.9537 - val_loss: 0.2957 - val_accuracy: 0.8962\n",
      "\n",
      "Finished training CNN model 4/4\n",
      "    n_total = 1.0e+05 samples\n",
      "         lr = 0.007\n",
      "Best test error rate was 9.9325% in epoch 11/20\n",
      "\n",
      "Total training time for 4 models: 5.44 mins\n"
     ]
    }
   ],
   "source": [
    "histories = {}\n",
    "t0_train = time.time()\n",
    "n_models = len(n_totals)*len(lrs)\n",
    "for i, (n_total, lr) in enumerate(itertools.product(n_totals, lrs)):\n",
    "  # train-test split\n",
    "  # grab first n_total samples from stim, and split them\n",
    "  # (Make sure the data type is np.int32 below, not idx_t!)\n",
    "  idxs_test, idxs_train = split_data(\n",
    "      np.arange(n_total, dtype=np.int32), \n",
    "      test_size=frac_test, \n",
    "      seed=12345, shuffle=False)\n",
    "  \n",
    "  # package stim output\n",
    "  data = {}\n",
    "  for label, index in zip([\"test\", \"train\"], \n",
    "                          [idxs_test, idxs_train]):\n",
    "      data[label] = [features_det_bits[index], \n",
    "                      features_det_evts[index],\n",
    "                      features_translation_map[index], \n",
    "                      features_final_det_evts[index] ]\n",
    "  \n",
    "  # train the model\n",
    "  print(f\"\\nTraining CNN model {i+1}/{n_models}\\n\"\n",
    "        f\"    n_total = {n_total:0.1e} samples\\n\" \n",
    "        f\"         lr = {lr}\\n\"\n",
    "        + \"-\"*28)\n",
    "  model_dxd = FullCNNModel(\n",
    "    observable_type, d, kernel_size, r,\n",
    "    [n_nodes for _ in range(2)], npol=2,\n",
    "    do_all_data_qubits=False, extended_kernel_output=True, \n",
    "    include_det_evts=True, include_last_kernel_dets=False, \n",
    "    include_last_dets=True, has_nonuniform_response=False, \n",
    "    use_translated_kernels=False\n",
    "  )\n",
    "  model_dxd.compile(\n",
    "     optimizer=Adam(learning_rate=lr), \n",
    "     loss='binary_crossentropy', metrics=['accuracy'])\n",
    "  history = model_dxd.fit(\n",
    "    x=data[\"train\"],\n",
    "    y=class_bits[idxs_train,:],\n",
    "    batch_size=batch_size,\n",
    "    epochs=n_epochs, \n",
    "    validation_data=(data[\"test\"], class_bits[idxs_test,:]),\n",
    "    callbacks=[\n",
    "      # tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5,\n",
    "      #                                 restore_best_weights=True),\n",
    "      # tf.keras.callbacks.LearningRateScheduler(learning_rate_scheduler)\n",
    "    ]\n",
    "  )\n",
    "\n",
    "  for metric, value in history.history.items():\n",
    "    histories[lr, n_total, metric] = value\n",
    "  \n",
    "  # evaluate the model\n",
    "  print(f\"\\nFinished training CNN model {i+1}/{n_models}\\n\"\n",
    "        f\"    n_total = {n_total:0.1e} samples\\n\" \n",
    "        f\"         lr = {lr}\")\n",
    "  best_epoch = np.argmax(history.history['val_accuracy'])\n",
    "  best_er_percent = 100*(1 - history.history['val_accuracy'][best_epoch])\n",
    "  print(f\"Best test error rate was {best_er_percent:.4f}% \"\n",
    "        f\"in epoch {best_epoch + 1}/{n_epochs}\")\n",
    "\n",
    "# save histories\n",
    "histories_file = f\"{base}/{run}/histories.csv\"\n",
    "histories = pd.DataFrame(histories)\n",
    "histories.columns.names = [\"lr\", \"N\", \"metric\"]\n",
    "histories.to_csv(histories_file, index=False) \n",
    "\n",
    "t1_train = time.time()\n",
    "dt = (t1_train - t0_train)/60\n",
    "print(f\"\\nTotal training time for {n_models} models: {dt:.2f} mins\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <th colspan=\"4\" halign=\"left\">0.005</th>\n",
       "      <th colspan=\"4\" halign=\"left\">0.007</th>\n",
       "      <th colspan=\"4\" halign=\"left\">0.005</th>\n",
       "      <th colspan=\"4\" halign=\"left\">0.007</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N</th>\n",
       "      <th colspan=\"4\" halign=\"left\">10000</th>\n",
       "      <th colspan=\"4\" halign=\"left\">10000</th>\n",
       "      <th colspan=\"4\" halign=\"left\">100000</th>\n",
       "      <th colspan=\"4\" halign=\"left\">100000</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metric</th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.607094</td>\n",
       "      <td>0.6215</td>\n",
       "      <td>0.764707</td>\n",
       "      <td>0.76525</td>\n",
       "      <td>0.619394</td>\n",
       "      <td>0.6840</td>\n",
       "      <td>0.534548</td>\n",
       "      <td>0.765250</td>\n",
       "      <td>0.487629</td>\n",
       "      <td>0.77940</td>\n",
       "      <td>0.443035</td>\n",
       "      <td>0.781012</td>\n",
       "      <td>0.529056</td>\n",
       "      <td>0.74655</td>\n",
       "      <td>0.456352</td>\n",
       "      <td>0.771288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.618781</td>\n",
       "      <td>0.7855</td>\n",
       "      <td>0.537532</td>\n",
       "      <td>0.76525</td>\n",
       "      <td>0.499662</td>\n",
       "      <td>0.7855</td>\n",
       "      <td>0.508134</td>\n",
       "      <td>0.765250</td>\n",
       "      <td>0.412140</td>\n",
       "      <td>0.78715</td>\n",
       "      <td>0.387641</td>\n",
       "      <td>0.791812</td>\n",
       "      <td>0.434595</td>\n",
       "      <td>0.78760</td>\n",
       "      <td>0.415658</td>\n",
       "      <td>0.787975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.533600</td>\n",
       "      <td>0.7855</td>\n",
       "      <td>0.539016</td>\n",
       "      <td>0.76525</td>\n",
       "      <td>0.481377</td>\n",
       "      <td>0.7840</td>\n",
       "      <td>0.507273</td>\n",
       "      <td>0.765125</td>\n",
       "      <td>0.360567</td>\n",
       "      <td>0.81500</td>\n",
       "      <td>0.345060</td>\n",
       "      <td>0.836187</td>\n",
       "      <td>0.388474</td>\n",
       "      <td>0.79890</td>\n",
       "      <td>0.369709</td>\n",
       "      <td>0.814038</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "lr         0.005                                     0.007                     \\\n",
       "N         10000                                     10000                       \n",
       "metric      loss accuracy  val_loss val_accuracy      loss accuracy  val_loss   \n",
       "0       0.607094   0.6215  0.764707      0.76525  0.619394   0.6840  0.534548   \n",
       "1       0.618781   0.7855  0.537532      0.76525  0.499662   0.7855  0.508134   \n",
       "2       0.533600   0.7855  0.539016      0.76525  0.481377   0.7840  0.507273   \n",
       "\n",
       "lr                      0.005                                     0.007  \\\n",
       "N                      100000                                    100000   \n",
       "metric val_accuracy      loss accuracy  val_loss val_accuracy      loss   \n",
       "0          0.765250  0.487629  0.77940  0.443035     0.781012  0.529056   \n",
       "1          0.765250  0.412140  0.78715  0.387641     0.791812  0.434595   \n",
       "2          0.765125  0.360567  0.81500  0.345060     0.836187  0.388474   \n",
       "\n",
       "lr                                      \n",
       "N                                       \n",
       "metric accuracy  val_loss val_accuracy  \n",
       "0       0.74655  0.456352     0.771288  \n",
       "1       0.78760  0.415658     0.787975  \n",
       "2       0.79890  0.369709     0.814038  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "histories.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
